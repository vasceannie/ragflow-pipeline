This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-30T11:54:40.023Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
.python-version
api.md
compose-dev.yaml
pyproject.toml
ragflow_uploader.py
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
.venv/
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

================
File: .python-version
================
3.12

================
File: api.md
================
sidebar_position: 1 slug: /http_api_reference
HTTP API Reference
A complete reference for RAGFlow's RESTful API. Before proceeding, please ensure you have your RAGFlow API key ready for authentication.

DATASET MANAGEMENT
Create dataset
POST /api/v1/datasets

Creates a dataset.

Request
Method: POST
URL: /api/v1/datasets
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name": string
"avatar": string
"description": string
"language": string
"embedding_model": string
"permission": string
"chunk_method": string
"parser_config": object
Request example
curl --request POST \
     --url http://{address}/api/v1/datasets \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '{
      "name": "test_1"
      }'
Request parameters
"name": (Body parameter), string, Required
The unique name of the dataset to create. It must adhere to the following requirements:

Permitted characters include:
English letters (a-z, A-Z)
Digits (0-9)
"_" (underscore)
Must begin with an English letter or underscore.
Maximum 65,535 characters.
Case-insensitive.
"avatar": (Body parameter), string
Base64 encoding of the avatar.

"description": (Body parameter), string
A brief description of the dataset to create.

"language": (Body parameter), string
The language setting of the dataset to create. Available options:

"English" (default)
"Chinese"
"embedding_model": (Body parameter), string
The name of the embedding model to use. For example: "BAAI/bge-zh-v1.5"

"permission": (Body parameter), string
Specifies who can access the dataset to create. Available options:

"me": (Default) Only you can manage the dataset.
"team": All team members can manage the dataset.
"chunk_method": (Body parameter), enum<string>
The chunking method of the dataset to create. Available options:

"naive": General (default)
"manual": Manual
"qa": Q&A
"table": Table
"paper": Paper
"book": Book
"laws": Laws
"presentation": Presentation
"picture": Picture
"one": One
"knowledge_graph": Knowledge Graph
Ensure your LLM is properly configured on the Settings page before selecting this. Please also note that Knowledge Graph consumes a large number of Tokens!
"email": Email
"parser_config": (Body parameter), object
The configuration settings for the dataset parser. The attributes in this JSON object vary with the selected "chunk_method":

If "chunk_method" is "naive", the "parser_config" object contains the following attributes:
"chunk_token_count": Defaults to 128.
"layout_recognize": Defaults to true.
"html4excel": Indicates whether to convert Excel documents into HTML format. Defaults to false.
"delimiter": Defaults to "\n!?。；！？".
"task_page_size": Defaults to 12. For PDF only.
"raptor": Raptor-specific settings. Defaults to: {"use_raptor": false}.
If "chunk_method" is "qa", "manuel", "paper", "book", "laws", or "presentation", the "parser_config" object contains the following attribute:
"raptor": Raptor-specific settings. Defaults to: {"use_raptor": false}.
If "chunk_method" is "table", "picture", "one", or "email", "parser_config" is an empty JSON object.
If "chunk_method" is "knowledge_graph", the "parser_config" object contains the following attributes:
"chunk_token_count": Defaults to 128.
"delimiter": Defaults to "\n!?。；！？".
"entity_types": Defaults to ["organization","person","location","event","time"]
Response
Success:

{
    "code": 0,
    "data": {
        "avatar": null,
        "chunk_count": 0,
        "chunk_method": "naive",
        "create_date": "Thu, 24 Oct 2024 09:14:07 GMT",
        "create_time": 1729761247434,
        "created_by": "69736c5e723611efb51b0242ac120007",
        "description": null,
        "document_count": 0,
        "embedding_model": "BAAI/bge-large-zh-v1.5",
        "id": "527fa74891e811ef9c650242ac120006",
        "language": "English",
        "name": "test_1",
        "parser_config": {
            "chunk_token_num": 128,
            "delimiter": "\\n!?;。；！？",
            "html4excel": false,
            "layout_recognize": true,
            "raptor": {
                "user_raptor": false
            }
        },
        "permission": "me",
        "similarity_threshold": 0.2,
        "status": "1",
        "tenant_id": "69736c5e723611efb51b0242ac120007",
        "token_num": 0,
        "update_date": "Thu, 24 Oct 2024 09:14:07 GMT",
        "update_time": 1729761247434,
        "vector_similarity_weight": 0.3
    }
}
Failure:

{
    "code": 102,
    "message": "Duplicated knowledgebase name in creating dataset."
}
Delete datasets
DELETE /api/v1/datasets

Deletes datasets by ID.

Request
Method: DELETE
URL: /api/v1/datasets
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"ids": list[string]
Request example
curl --request DELETE \
     --url http://{address}/api/v1/datasets \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '{
     "ids": ["test_1", "test_2"]
     }'
Request parameters
"ids": (Body parameter), list[string]
The IDs of the datasets to delete. If it is not specified, all datasets will be deleted.
Response
Success:

{
    "code": 0 
}
Failure:

{
    "code": 102,
    "message": "You don't own the dataset."
}
Update dataset
PUT /api/v1/datasets/{dataset_id}

Updates configurations for a specified dataset.

Request
Method: PUT
URL: /api/v1/datasets/{dataset_id}
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name": string
"embedding_model": string
"chunk_method": enum<string>
Request example
curl --request PUT \
     --url http://{address}/api/v1/datasets/{dataset_id} \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "name": "updated_dataset"
     }'
Request parameters
dataset_id: (Path parameter)
The ID of the dataset to update.
"name": (Body parameter), string
The revised name of the dataset.
"embedding_model": (Body parameter), string
The updated embedding model name.
Ensure that "chunk_count" is 0 before updating "embedding_model".
"chunk_method": (Body parameter), enum<string>
The chunking method for the dataset. Available options:
"naive": General
"manual: Manual
"qa": Q&A
"table": Table
"paper": Paper
"book": Book
"laws": Laws
"presentation": Presentation
"picture": Picture
"one":One
"email": Email
"knowledge_graph": Knowledge Graph
Ensure your LLM is properly configured on the Settings page before selecting this. Please also note that Knowledge Graph consumes a large number of Tokens!
Response
Success:

{
    "code": 0 
}
Failure:

{
    "code": 102,
    "message": "Can't change tenant_id."
}
List datasets
GET /api/v1/datasets?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={dataset_name}&id={dataset_id}

Lists datasets.

Request
Method: GET
URL: /api/v1/datasets?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={dataset_name}&id={dataset_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/datasets?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={dataset_name}&id={dataset_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>'
Request parameters
page: (Filter parameter)
Specifies the page on which the datasets will be displayed. Defaults to 1.
page_size: (Filter parameter)
The number of datasets on each page. Defaults to 30.
orderby: (Filter parameter)
The field by which datasets should be sorted. Available options:
create_time (default)
update_time
desc: (Filter parameter)
Indicates whether the retrieved datasets should be sorted in descending order. Defaults to true.
name: (Filter parameter)
The name of the dataset to retrieve.
id: (Filter parameter)
The ID of the dataset to retrieve.
Response
Success:

{
    "code": 0,
    "data": [
        {
            "avatar": "",
            "chunk_count": 59,
            "create_date": "Sat, 14 Sep 2024 01:12:37 GMT",
            "create_time": 1726276357324,
            "created_by": "69736c5e723611efb51b0242ac120007",
            "description": null,
            "document_count": 1,
            "embedding_model": "BAAI/bge-large-zh-v1.5",
            "id": "6e211ee0723611efa10a0242ac120007",
            "language": "English",
            "name": "mysql",
            "chunk_method": "knowledge_graph",
            "parser_config": {
                "chunk_token_num": 8192,
                "delimiter": "\\n!?;。；！？",
                "entity_types": [
                    "organization",
                    "person",
                    "location",
                    "event",
                    "time"
                ]
            },
            "permission": "me",
            "similarity_threshold": 0.2,
            "status": "1",
            "tenant_id": "69736c5e723611efb51b0242ac120007",
            "token_num": 12744,
            "update_date": "Thu, 10 Oct 2024 04:07:23 GMT",
            "update_time": 1728533243536,
            "vector_similarity_weight": 0.3
        }
    ]
}
Failure:

{
    "code": 102,
    "message": "The dataset doesn't exist"
}
FILE MANAGEMENT WITHIN DATASET
Upload documents
POST /api/v1/datasets/{dataset_id}/documents

Uploads documents to a specified dataset.

Request
Method: POST
URL: /api/v1/datasets/{dataset_id}/documents
Headers:
'Content-Type: multipart/form-data'
'Authorization: Bearer <YOUR_API_KEY>'
Form:
'file=@{FILE_PATH}'
Request example
curl --request POST \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents \
     --header 'Content-Type: multipart/form-data' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --form 'file=@./test1.txt' \
     --form 'file=@./test2.pdf'
Request parameters
dataset_id: (Path parameter)
The ID of the dataset to which the documents will be uploaded.
'file': (Body parameter)
A document to upload.
Response
Success:

{
    "code": 0,
    "data": [
        {
            "chunk_method": "naive",
            "created_by": "69736c5e723611efb51b0242ac120007",
            "dataset_id": "527fa74891e811ef9c650242ac120006",
            "id": "b330ec2e91ec11efbc510242ac120004",
            "location": "1.txt",
            "name": "1.txt",
            "parser_config": {
                "chunk_token_num": 128,
                "delimiter": "\\n!?;。；！？",
                "html4excel": false,
                "layout_recognize": true,
                "raptor": {
                    "user_raptor": false
                }
            },
            "run": "UNSTART",
            "size": 17966,
            "thumbnail": "",
            "type": "doc"
        }
    ]
}
Failure:

{
    "code": 101,
    "message": "No file part!"
}
Update document
PUT /api/v1/datasets/{dataset_id}/documents/{document_id}

Updates configurations for a specified document.

Request
Method: PUT
URL: /api/v1/datasets/{dataset_id}/documents/{document_id}
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name":string
"chunk_method":string
"parser_config":object
Request example
curl --request PUT \
     --url http://{address}/api/v1/datasets/{dataset_id}/info/{document_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --header 'Content-Type: application/json' \
     --data '
     {
          "name": "manual.txt", 
          "chunk_method": "manual", 
          "parser_config": {"chunk_token_count": 128}
     }'

Request parameters
dataset_id: (Path parameter)
The ID of the associated dataset.
document_id: (Path parameter)
The ID of the document to update.
"name": (Body parameter), string
"chunk_method": (Body parameter), string
The parsing method to apply to the document:
"naive": General
"manual: Manual
"qa": Q&A
"table": Table
"paper": Paper
"book": Book
"laws": Laws
"presentation": Presentation
"picture": Picture
"one": One
"knowledge_graph": Knowledge Graph
Ensure your LLM is properly configured on the Settings page before selecting this. Please also note that Knowledge Graph consumes a large number of Tokens!
"email": Email
"parser_config": (Body parameter), object
The configuration settings for the dataset parser. The attributes in this JSON object vary with the selected "chunk_method":
If "chunk_method" is "naive", the "parser_config" object contains the following attributes:
"chunk_token_count": Defaults to 128.
"layout_recognize": Defaults to true.
"html4excel": Indicates whether to convert Excel documents into HTML format. Defaults to false.
"delimiter": Defaults to "\n!?。；！？".
"task_page_size": Defaults to 12. For PDF only.
"raptor": Raptor-specific settings. Defaults to: {"use_raptor": false}.
If "chunk_method" is "qa", "manuel", "paper", "book", "laws", or "presentation", the "parser_config" object contains the following attribute:
"raptor": Raptor-specific settings. Defaults to: {"use_raptor": false}.
If "chunk_method" is "table", "picture", "one", or "email", "parser_config" is an empty JSON object.
If "chunk_method" is "knowledge_graph", the "parser_config" object contains the following attributes:
"chunk_token_count": Defaults to 128.
"delimiter": Defaults to "\n!?。；！？".
"entity_types": Defaults to ["organization","person","location","event","time"]
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "The dataset does not have the document."
}
Download document
GET /api/v1/datasets/{dataset_id}/documents/{document_id}

Downloads a document from a specified dataset.

Request
Method: GET
URL: /api/v1/datasets/{dataset_id}/documents/{document_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Output:
'{PATH_TO_THE_FILE}'
Request example
curl --request GET \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents/{document_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --output ./ragflow.txt
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
documents_id: (Path parameter)
The ID of the document to download.
Response
Success:

This is a test to verify the file download feature.
Failure:

{
    "code": 102,
    "message": "You do not own the dataset 7898da028a0511efbf750242ac1220005."
}
List documents
GET /api/v1/datasets/{dataset_id}/documents?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&keywords={keywords}&id={document_id}&name={document_name}

Lists documents in a specified dataset.

Request
Method: GET
URL: /api/v1/datasets/{dataset_id}/documents?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&keywords={keywords}&id={document_id}&name={document_name}
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&keywords={keywords}&id={document_id}&name={document_name} \
     --header 'Authorization: Bearer <YOUR_API_KEY>'
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
keywords: (Filter parameter), string
The keywords used to match document titles.
page: (Filter parameter), integer Specifies the page on which the documents will be displayed. Defaults to 1.
page_size: (Filter parameter), integer
The maximum number of documents on each page. Defaults to 30.
orderby: (Filter parameter), string
The field by which documents should be sorted. Available options:
create_time (default)
update_time
desc: (Filter parameter), boolean
Indicates whether the retrieved documents should be sorted in descending order. Defaults to true.
id: (Filter parameter), string
The ID of the document to retrieve.
Response
Success:

{
    "code": 0,
    "data": {
        "docs": [
            {
                "chunk_count": 0,
                "create_date": "Mon, 14 Oct 2024 09:11:01 GMT",
                "create_time": 1728897061948,
                "created_by": "69736c5e723611efb51b0242ac120007",
                "id": "3bcfbf8a8a0c11ef8aba0242ac120006",
                "knowledgebase_id": "7898da028a0511efbf750242ac120005",
                "location": "Test_2.txt",
                "name": "Test_2.txt",
                "parser_config": {
                    "chunk_token_count": 128,
                    "delimiter": "\n!?。；！？",
                    "layout_recognize": true,
                    "task_page_size": 12
                },
                "chunk_method": "naive",
                "process_begin_at": null,
                "process_duation": 0.0,
                "progress": 0.0,
                "progress_msg": "",
                "run": "0",
                "size": 7,
                "source_type": "local",
                "status": "1",
                "thumbnail": null,
                "token_count": 0,
                "type": "doc",
                "update_date": "Mon, 14 Oct 2024 09:11:01 GMT",
                "update_time": 1728897061948
            }
        ],
        "total": 1
    }
}
Failure:

{
    "code": 102,
    "message": "You don't own the dataset 7898da028a0511efbf750242ac1220005. "
}
Delete documents
DELETE /api/v1/datasets/{dataset_id}/documents

Deletes documents by ID.

Request
Method: DELETE
URL: /api/v1/datasets/{dataset_id}/documents
Headers:
'Content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"ids": list[string]
Request example
curl --request DELETE \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "ids": ["id_1","id_2"]
     }'
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
"ids": (Body parameter), list[string]
The IDs of the documents to delete. If it is not specified, all documents in the specified dataset will be deleted.
Response
Success:

{
    "code": 0
}.
Failure:

{
    "code": 102,
    "message": "You do not own the dataset 7898da028a0511efbf750242ac1220005."
}
Parse documents
POST /api/v1/datasets/{dataset_id}/chunks

Parses documents in a specified dataset.

Request
Method: POST
URL: /api/v1/datasets/{dataset_id}/chunks
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"document_ids": list[string]
Request example
curl --request POST \
     --url http://{address}/api/v1/datasets/{dataset_id}/chunks \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "document_ids": ["97a5f1c2759811efaa500242ac120004","97ad64b6759811ef9fc30242ac120004"]
     }'
Request parameters
dataset_id: (Path parameter)
The dataset ID.
"document_ids": (Body parameter), list[string], Required
The IDs of the documents to parse.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "`document_ids` is required"
}
Stop parsing documents
DELETE /api/v1/datasets/{dataset_id}/chunks

Stops parsing specified documents.

Request
Method: DELETE
URL: /api/v1/datasets/{dataset_id}/chunks
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"document_ids": list[string]
Request example
curl --request DELETE \
     --url http://{address}/api/v1/datasets/{dataset_id}/chunks \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "document_ids": ["97a5f1c2759811efaa500242ac120004","97ad64b6759811ef9fc30242ac120004"]
     }'
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
"document_ids": (Body parameter), list[string], Required
The IDs of the documents for which the parsing should be stopped.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "`document_ids` is required"
}
Add chunk
POST /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks

Adds a chunk to a specified document in a specified dataset.

Request
Method: POST
URL: /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"content": string
"important_keywords": list[string]
Request example
curl --request POST \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "content": "<CHUNK_CONTENT_HERE>"
     }'
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
document_ids: (Path parameter)
The associated document ID.
"content": (Body parameter), string, Required
The text content of the chunk.
"important_keywords(Body parameter), list[string]
The key terms or phrases to tag with the chunk.
Response
Success:

{
    "code": 0,
    "data": {
        "chunk": {
            "content": "ragflow content",
            "create_time": "2024-10-16 08:05:04",
            "create_timestamp": 1729065904.581025,
            "dataset_id": [
                "c7ee74067a2c11efb21c0242ac120006"
            ],
            "document_id": "5c5999ec7be811ef9cab0242ac120005",
            "id": "d78435d142bd5cf6704da62c778795c5",
            "important_keywords": []
        }
    }
}
Failure:

{
    "code": 102,
    "message": "`content` is required"
}
List chunks
GET /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks?keywords={keywords}&page={page}&page_size={page_size}&id={id}

Lists chunks in a specified document.

Request
Method: GET
URL: /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks?keywords={keywords}&page={page}&page_size={page_size}&id={chunk_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks?keywords={keywords}&page={page}&page_size={page_size}&id={chunk_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>' 
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
document_ids: (Path parameter)
The associated document ID.
keywords(Filter parameter), string
The keywords used to match chunk content.
page(Filter parameter), integer
Specifies the page on which the chunks will be displayed. Defaults to 1.
page_size(Filter parameter), integer
The maximum number of chunks on each page. Defaults to 1024.
id(Filter parameter), string
The ID of the chunk to retrieve.
Response
Success:

{
    "code": 0,
    "data": {
        "chunks": [
            {
                "available_int": 1,
                "content": "This is a test content.",
                "docnm_kwd": "1.txt",
                "document_id": "b330ec2e91ec11efbc510242ac120004",
                "id": "b48c170e90f70af998485c1065490726",
                "image_id": "",
                "important_keywords": "",
                "positions": [
                    ""
                ]
            }
        ],
        "doc": {
            "chunk_count": 1,
            "chunk_method": "naive",
            "create_date": "Thu, 24 Oct 2024 09:45:27 GMT",
            "create_time": 1729763127646,
            "created_by": "69736c5e723611efb51b0242ac120007",
            "dataset_id": "527fa74891e811ef9c650242ac120006",
            "id": "b330ec2e91ec11efbc510242ac120004",
            "location": "1.txt",
            "name": "1.txt",
            "parser_config": {
                "chunk_token_num": 128,
                "delimiter": "\\n!?;。；！？",
                "html4excel": false,
                "layout_recognize": true,
                "raptor": {
                    "user_raptor": false
                }
            },
            "process_begin_at": "Thu, 24 Oct 2024 09:56:44 GMT",
            "process_duation": 0.54213,
            "progress": 0.0,
            "progress_msg": "Task dispatched...",
            "run": "2",
            "size": 17966,
            "source_type": "local",
            "status": "1",
            "thumbnail": "",
            "token_count": 8,
            "type": "doc",
            "update_date": "Thu, 24 Oct 2024 11:03:15 GMT",
            "update_time": 1729767795721
        },
        "total": 1
    }
}
Failure:

{
    "code": 102,
    "message": "You don't own the document 5c5999ec7be811ef9cab0242ac12000e5."
}
Delete chunks
DELETE /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks

Deletes chunks by ID.

Request
Method: DELETE
URL: /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"chunk_ids": list[string]
Request example
curl --request DELETE \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "chunk_ids": ["test_1", "test_2"]
     }'
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
document_ids: (Path parameter)
The associated document ID.
"chunk_ids": (Body parameter), list[string]
The IDs of the chunks to delete. If it is not specified, all chunks of the specified document will be deleted.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "`chunk_ids` is required"
}
Update chunk
PUT /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks/{chunk_id}

Updates content or configurations for a specified chunk.

Request
Method: PUT
URL: /api/v1/datasets/{dataset_id}/documents/{document_id}/chunks/{chunk_id}
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"content": string
"important_keywords": list[string]
"available": boolean
Request example
curl --request PUT \
     --url http://{address}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks/{chunk_id} \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {   
          "content": "ragflow123",  
          "important_keywords": []  
     }'
Request parameters
dataset_id: (Path parameter)
The associated dataset ID.
document_ids: (Path parameter)
The associated document ID.
chunk_id: (Path parameter)
The ID of the chunk to update.
"content": (Body parameter), string
The text content of the chunk.
"important_keywords": (Body parameter), list[string]
A list of key terms or phrases to tag with the chunk.
"available": (Body parameter) boolean
The chunk's availability status in the dataset. Value options:
true: Available (default)
false: Unavailable
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "Can't find this chunk 29a2d9987e16ba331fb4d7d30d99b71d2"
}
Retrieve chunks
POST /api/v1/retrieval

Retrieves chunks from specified datasets.

Request
Method: POST
URL: /api/v1/retrieval
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"question": string
"dataset_ids": list[string]
"document_ids": list[string]
"page": integer
"page_size": integer
"similarity_threshold": float
"vector_similarity_weight": float
"top_k": integer
"rerank_id": string
"keyword": boolean
"highlight": boolean
Request example
curl --request POST \
     --url http://{address}/api/v1/retrieval \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "question": "What is advantage of ragflow?",
          "dataset_ids": ["b2a62730759d11ef987d0242ac120004"],
          "document_ids": ["77df9ef4759a11ef8bdd0242ac120004"]
     }'
Request parameter
"question": (Body parameter), string, Required
The user query or query keywords.
"dataset_ids": (Body parameter) list[string]
The IDs of the datasets to search. If you do not set this argument, ensure that you set "document_ids".
"document_ids": (Body parameter), list[string]
The IDs of the documents to search. Ensure that all selected documents use the same embedding model. Otherwise, an error will occur. If you do not set this argument, ensure that you set "dataset_ids".
"page": (Body parameter), integer
Specifies the page on which the chunks will be displayed. Defaults to 1.
"page_size": (Body parameter)
The maximum number of chunks on each page. Defaults to 30.
"similarity_threshold": (Body parameter)
The minimum similarity score. Defaults to 0.2.
"vector_similarity_weight": (Body parameter), float
The weight of vector cosine similarity. Defaults to 0.3. If x represents the weight of vector cosine similarity, then (1 - x) is the term similarity weight.
"top_k": (Body parameter), integer
The number of chunks engaged in vector cosine computaton. Defaults to 1024.
"rerank_id": (Body parameter), integer
The ID of the rerank model.
"keyword": (Body parameter), boolean
Indicates whether to enable keyword-based matching:
true: Enable keyword-based matching.
false: Disable keyword-based matching (default).
"highlight": (Body parameter), boolean
Specifies whether to enable highlighting of matched terms in the results:
true: Enable highlighting of matched terms.
false: Disable highlighting of matched terms (default).
Response
Success:

{
    "code": 0,
    "data": {
        "chunks": [
            {
                "content": "ragflow content",
                "content_ltks": "ragflow content",
                "document_id": "5c5999ec7be811ef9cab0242ac120005",
                "document_keyword": "1.txt",
                "highlight": "<em>ragflow</em> content",
                "id": "d78435d142bd5cf6704da62c778795c5",
                "image_id": "",
                "important_keywords": [
                    ""
                ],
                "kb_id": "c7ee74067a2c11efb21c0242ac120006",
                "positions": [
                    ""
                ],
                "similarity": 0.9669436601210759,
                "term_similarity": 1.0,
                "vector_similarity": 0.8898122004035864
            }
        ],
        "doc_aggs": [
            {
                "count": 1,
                "doc_id": "5c5999ec7be811ef9cab0242ac120005",
                "doc_name": "1.txt"
            }
        ],
        "total": 1
    }
}
Failure:

{
    "code": 102,
    "message": "`datasets` is required."
}
CHAT ASSISTANT MANAGEMENT
Create chat assistant
POST /api/v1/chats

Creates a chat assistant.

Request
Method: POST
URL: /api/v1/chats
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name": string
"avatar": string
"dataset_ids": list[string]
"llm": object
"prompt": object
Request example
curl --request POST \
     --url http://{address}/api/v1/chats \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '{
    "dataset_ids": ["0b2cbc8c877f11ef89070242ac120005"],
    "name":"new_chat_1"
}'
Request parameters
"name": (Body parameter), string, Required
The name of the chat assistant.
"avatar": (Body parameter), string
Base64 encoding of the avatar.
"dataset_ids": (Body parameter), list[string]
The IDs of the associated datasets.
"llm": (Body parameter), object
The LLM settings for the chat assistant to create. If it is not explicitly set, a JSON object with the following values will be generated as the default. An llm JSON object contains the following attributes:
"model_name", string
The chat model name. If not set, the user's default chat model will be used.
"temperature": float
Controls the randomness of the model's predictions. A lower temperature results in more conservative responses, while a higher temperature yields more creative and diverse responses. Defaults to 0.1.
"top_p": float
Also known as “nucleus sampling”, this parameter sets a threshold to select a smaller set of words to sample from. It focuses on the most likely words, cutting off the less probable ones. Defaults to 0.3
"presence_penalty": float
This discourages the model from repeating the same information by penalizing words that have already appeared in the conversation. Defaults to 0.2.
"frequency penalty": float
Similar to the presence penalty, this reduces the model’s tendency to repeat the same words frequently. Defaults to 0.7.
"max_token": integer
The maximum length of the model's output, measured in the number of tokens (words or pieces of words). Defaults to 512. If disabled, you lift the maximum token limit, allowing the model to determine the number of tokens in its responses.
"prompt": (Body parameter), object
Instructions for the LLM to follow. If it is not explicitly set, a JSON object with the following values will be generated as the default. A prompt JSON object contains the following attributes:
"similarity_threshold": float RAGFlow employs either a combination of weighted keyword similarity and weighted vector cosine similarity, or a combination of weighted keyword similarity and weighted reranking score during retrieval. This argument sets the threshold for similarities between the user query and chunks. If a similarity score falls below this threshold, the corresponding chunk will be excluded from the results. The default value is 0.2.
"keywords_similarity_weight": float This argument sets the weight of keyword similarity in the hybrid similarity score with vector cosine similarity or reranking model similarity. By adjusting this weight, you can control the influence of keyword similarity in relation to other similarity measures. The default value is 0.7.
"top_n": int This argument specifies the number of top chunks with similarity scores above the similarity_threshold that are fed to the LLM. The LLM will only access these 'top N' chunks. The default value is 8.
"variables": object[] This argument lists the variables to use in the 'System' field of Chat Configurations. Note that:
"knowledge" is a reserved variable, which represents the retrieved chunks.
All the variables in 'System' should be curly bracketed.
The default value is [{"key": "knowledge", "optional": true}].
"rerank_model": string If it is not specified, vector cosine similarity will be used; otherwise, reranking score will be used.
"empty_response": string If nothing is retrieved in the dataset for the user's question, this will be used as the response. To allow the LLM to improvise when nothing is found, leave this blank.
"opener": string The opening greeting for the user. Defaults to "Hi! I am your assistant, can I help you?".
"show_quote: boolean Indicates whether the source of text should be displayed. Defaults to true.
"prompt": string The prompt content.
Response
Success:

{
    "code": 0,
    "data": {
        "avatar": "",
        "create_date": "Thu, 24 Oct 2024 11:18:29 GMT",
        "create_time": 1729768709023,
        "dataset_ids": [
            "527fa74891e811ef9c650242ac120006"
        ],
        "description": "A helpful Assistant",
        "do_refer": "1",
        "id": "b1f2f15691f911ef81180242ac120003",
        "language": "English",
        "llm": {
            "frequency_penalty": 0.7,
            "max_tokens": 512,
            "model_name": "qwen-plus@Tongyi-Qianwen",
            "presence_penalty": 0.4,
            "temperature": 0.1,
            "top_p": 0.3
        },
        "name": "12234",
        "prompt": {
            "empty_response": "Sorry! No relevant content was found in the knowledge base!",
            "keywords_similarity_weight": 0.3,
            "opener": "Hi! I'm your assistant, what can I do for you?",
            "prompt": "You are an intelligent assistant. Please summarize the content of the knowledge base to answer the question. Please list the data in the knowledge base and answer in detail. When all knowledge base content is irrelevant to the question, your answer must include the sentence \"The answer you are looking for is not found in the knowledge base!\" Answers need to consider chat history.\n ",
            "rerank_model": "",
            "similarity_threshold": 0.2,
            "top_n": 6,
            "variables": [
                {
                    "key": "knowledge",
                    "optional": false
                }
            ]
        },
        "prompt_type": "simple",
        "status": "1",
        "tenant_id": "69736c5e723611efb51b0242ac120007",
        "top_k": 1024,
        "update_date": "Thu, 24 Oct 2024 11:18:29 GMT",
        "update_time": 1729768709023
    }
}
Failure:

{
    "code": 102,
    "message": "Duplicated chat name in creating dataset."
}
Update chat assistant
PUT /api/v1/chats/{chat_id}

Updates configurations for a specified chat assistant.

Request
Method: PUT
URL: /api/v1/chats/{chat_id}
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name": string
"avatar": string
"dataset_ids": list[string]
"llm": object
"prompt": object
Request example
curl --request PUT \
     --url http://{address}/api/v1/chats/{chat_id} \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "name":"Test"
     }'
Parameters
chat_id: (Path parameter)
The ID of the chat assistant to update.
"name": (Body parameter), string, Required
The revised name of the chat assistant.
"avatar": (Body parameter), string
Base64 encoding of the avatar.
"dataset_ids": (Body parameter), list[string]
The IDs of the associated datasets.
"llm": (Body parameter), object
The LLM settings for the chat assistant to create. If it is not explicitly set, a dictionary with the following values will be generated as the default. An llm object contains the following attributes:
"model_name", string
The chat model name. If not set, the user's default chat model will be used.
"temperature": float
Controls the randomness of the model's predictions. A lower temperature results in more conservative responses, while a higher temperature yields more creative and diverse responses. Defaults to 0.1.
"top_p": float
Also known as “nucleus sampling”, this parameter sets a threshold to select a smaller set of words to sample from. It focuses on the most likely words, cutting off the less probable ones. Defaults to 0.3
"presence_penalty": float
This discourages the model from repeating the same information by penalizing words that have already appeared in the conversation. Defaults to 0.2.
"frequency penalty": float
Similar to the presence penalty, this reduces the model’s tendency to repeat the same words frequently. Defaults to 0.7.
"max_token": integer
The maximum length of the model's output, measured in the number of tokens (words or pieces of words). Defaults to 512. If disabled, you lift the maximum token limit, allowing the model to determine the number of tokens in its responses.
"prompt": (Body parameter), object
Instructions for the LLM to follow. A prompt object contains the following attributes:
"similarity_threshold": float RAGFlow employs either a combination of weighted keyword similarity and weighted vector cosine similarity, or a combination of weighted keyword similarity and weighted rerank score during retrieval. This argument sets the threshold for similarities between the user query and chunks. If a similarity score falls below this threshold, the corresponding chunk will be excluded from the results. The default value is 0.2.
"keywords_similarity_weight": float This argument sets the weight of keyword similarity in the hybrid similarity score with vector cosine similarity or reranking model similarity. By adjusting this weight, you can control the influence of keyword similarity in relation to other similarity measures. The default value is 0.7.
"top_n": int This argument specifies the number of top chunks with similarity scores above the similarity_threshold that are fed to the LLM. The LLM will only access these 'top N' chunks. The default value is 8.
"variables": object[] This argument lists the variables to use in the 'System' field of Chat Configurations. Note that:
"knowledge" is a reserved variable, which represents the retrieved chunks.
All the variables in 'System' should be curly bracketed.
The default value is [{"key": "knowledge", "optional": true}]
"rerank_model": string If it is not specified, vector cosine similarity will be used; otherwise, reranking score will be used.
"empty_response": string If nothing is retrieved in the dataset for the user's question, this will be used as the response. To allow the LLM to improvise when nothing is found, leave this blank.
"opener": string The opening greeting for the user. Defaults to "Hi! I am your assistant, can I help you?".
"show_quote: boolean Indicates whether the source of text should be displayed. Defaults to true.
"prompt": string The prompt content.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "Duplicated chat name in updating dataset."
}
Delete chat assistants
DELETE /api/v1/chats

Deletes chat assistants by ID.

Request
Method: DELETE
URL: /api/v1/chats
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"ids": list[string]
Request example
curl --request DELETE \
     --url http://{address}/api/v1/chats \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "ids": ["test_1", "test_2"]
     }'
Request parameters
"ids": (Body parameter), list[string]
The IDs of the chat assistants to delete. If it is not specified, all chat assistants in the system will be deleted.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "ids are required"
}
List chat assistants
GET /api/v1/chats?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={chat_name}&id={chat_id}

Lists chat assistants.

Request
Method: GET
URL: /api/v1/chats?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={dataset_name}&id={dataset_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/chats?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={dataset_name}&id={dataset_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>'
Request parameters
page: (Filter parameter), integer
Specifies the page on which the chat assistants will be displayed. Defaults to 1.
page_size: (Filter parameter), integer
The number of chat assistants on each page. Defaults to 30.
orderby: (Filter parameter), string
The attribute by which the results are sorted. Available options:
create_time (default)
update_time
desc: (Filter parameter), boolean
Indicates whether the retrieved chat assistants should be sorted in descending order. Defaults to true.
id: (Filter parameter), string
The ID of the chat assistant to retrieve.
name: (Filter parameter), string
The name of the chat assistant to retrieve.
Response
Success:

{
    "code": 0,
    "data": [
        {
            "avatar": "",
            "create_date": "Fri, 18 Oct 2024 06:20:06 GMT",
            "create_time": 1729232406637,
            "description": "A helpful Assistant",
            "do_refer": "1",
            "id": "04d0d8e28d1911efa3630242ac120006",
            "dataset_ids": ["527fa74891e811ef9c650242ac120006"],
            "language": "English",
            "llm": {
                "frequency_penalty": 0.7,
                "max_tokens": 512,
                "model_name": "qwen-plus@Tongyi-Qianwen",
                "presence_penalty": 0.4,
                "temperature": 0.1,
                "top_p": 0.3
            },
            "name": "13243",
            "prompt": {
                "empty_response": "Sorry! No relevant content was found in the knowledge base!",
                "keywords_similarity_weight": 0.3,
                "opener": "Hi! I'm your assistant, what can I do for you?",
                "prompt": "You are an intelligent assistant. Please summarize the content of the knowledge base to answer the question. Please list the data in the knowledge base and answer in detail. When all knowledge base content is irrelevant to the question, your answer must include the sentence \"The answer you are looking for is not found in the knowledge base!\" Answers need to consider chat history.\n",
                "rerank_model": "",
                "similarity_threshold": 0.2,
                "top_n": 6,
                "variables": [
                    {
                        "key": "knowledge",
                        "optional": false
                    }
                ]
            },
            "prompt_type": "simple",
            "status": "1",
            "tenant_id": "69736c5e723611efb51b0242ac120007",
            "top_k": 1024,
            "update_date": "Fri, 18 Oct 2024 06:20:06 GMT",
            "update_time": 1729232406638
        }
    ]
}
Failure:

{
    "code": 102,
    "message": "The chat doesn't exist"
}
SESSION MANAGEMENT
Create session with chat assistant
POST /api/v1/chats/{chat_id}/sessions

Creates a session with a chat assistant.

Request
Method: POST
URL: /api/v1/chats/{chat_id}/sessions
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name": string
"user_id": string(optional)
Request example
curl --request POST \
     --url http://{address}/api/v1/chats/{chat_id}/sessions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "name": "new session"
     }'
Request parameters
chat_id: (Path parameter)
The ID of the associated chat assistant.
"name": (Body parameter), string
The name of the chat session to create.
"user_id": (Body parameter), string
Optional user-defined ID.
Response
Success:

{
    "code": 0,
    "data": {
        "chat_id": "2ca4b22e878011ef88fe0242ac120005",
        "create_date": "Fri, 11 Oct 2024 08:46:14 GMT",
        "create_time": 1728636374571,
        "id": "4606b4ec87ad11efbc4f0242ac120006",
        "messages": [
            {
                "content": "Hi! I am your assistant，can I help you?",
                "role": "assistant"
            }
        ],
        "name": "new session",
        "update_date": "Fri, 11 Oct 2024 08:46:14 GMT",
        "update_time": 1728636374571
    }
}
Failure:

{
    "code": 102,
    "message": "Name cannot be empty."
}
Update chat assistant's session
PUT /api/v1/chats/{chat_id}/sessions/{session_id}

Updates a session of a specified chat assistant.

Request
Method: PUT
URL: /api/v1/chats/{chat_id}/sessions/{session_id}
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"name: string
"user_id: string(optional)
Request example
curl --request PUT \
     --url http://{address}/api/v1/chats/{chat_id}/sessions/{session_id} \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "name": "<REVISED_SESSION_NAME_HERE>"
     }'
Request Parameter
chat_id: (Path parameter)
The ID of the associated chat assistant.
session_id: (Path parameter)
The ID of the session to update.
"name": (Body Parameter), string
The revised name of the session.
"user_id": (Body parameter), string
Optional user-defined ID.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "Name cannot be empty."
}
List chat assistant's sessions
GET /api/v1/chats/{chat_id}/sessions?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={session_name}&id={session_id}

Lists sessions associated with a specified chat assistant.

Request
Method: GET
URL: /api/v1/chats/{chat_id}/sessions?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={session_name}&id={session_id}&user_id={user_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/chats/{chat_id}/sessions?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={session_name}&id={session_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>'
Request Parameters
chat_id: (Path parameter)
The ID of the associated chat assistant.
page: (Filter parameter), integer
Specifies the page on which the sessions will be displayed. Defaults to 1.
page_size: (Filter parameter), integer
The number of sessions on each page. Defaults to 30.
orderby: (Filter parameter), string
The field by which sessions should be sorted. Available options:
create_time (default)
update_time
desc: (Filter parameter), boolean
Indicates whether the retrieved sessions should be sorted in descending order. Defaults to true.
name: (Filter parameter) string
The name of the chat session to retrieve.
id: (Filter parameter), string
The ID of the chat session to retrieve.
user_id: (Filter parameter), string
The optional user-defined ID passed in when creating session.
Response
Success:

{
    "code": 0,
    "data": [
        {
            "chat": "2ca4b22e878011ef88fe0242ac120005",
            "create_date": "Fri, 11 Oct 2024 08:46:43 GMT",
            "create_time": 1728636403974,
            "id": "578d541e87ad11ef96b90242ac120006",
            "messages": [
                {
                    "content": "Hi! I am your assistant，can I help you?",
                    "role": "assistant"
                }
            ],
            "name": "new session",
            "update_date": "Fri, 11 Oct 2024 08:46:43 GMT",
            "update_time": 1728636403974
        }
    ]
}
Failure:

{
    "code": 102,
    "message": "The session doesn't exist"
}
Delete chat assistant's sessions
DELETE /api/v1/chats/{chat_id}/sessions

Deletes sessions of a chat assistant by ID.

Request
Method: DELETE
URL: /api/v1/chats/{chat_id}/sessions
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"ids": list[string]
Request example
curl --request DELETE \
     --url http://{address}/api/v1/chats/{chat_id}/sessions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '
     {
          "ids": ["test_1", "test_2"]
     }'
Request Parameters
chat_id: (Path parameter)
The ID of the associated chat assistant.
"ids": (Body Parameter), list[string]
The IDs of the sessions to delete. If it is not specified, all sessions associated with the specified chat assistant will be deleted.
Response
Success:

{
    "code": 0
}
Failure:

{
    "code": 102,
    "message": "The chat doesn't own the session"
}
Converse with chat assistant
POST /api/v1/chats/{chat_id}/completions

Asks a specified chat assistant a question to start an AI-powered conversation.

:::tip NOTE

In streaming mode, not all responses include a reference, as this depends on the system's judgement.
In streaming mode, the last message is an empty message:
data:
{
  "code": 0,
  "data": true
}
:::

Request
Method: POST
URL: /api/v1/chats/{chat_id}/completions
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"question": string
"stream": boolean
"session_id": string(optional)
"user_id: string(optional)
Request example
curl --request POST \
     --url http://{address}/api/v1/chats/{chat_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data-binary '
     {
     }'
curl --request POST \
     --url http://{address}/api/v1/chats/{chat_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data-binary '
     {
          "question": "Who are you",
          "stream": true,
          "session_id":"9fa7691cb85c11ef9c5f0242ac120005"
     }'
Request Parameters
chat_id: (Path parameter)
The ID of the associated chat assistant.
"question": (Body Parameter), string, Required
The question to start an AI-powered conversation.
"stream": (Body Parameter), boolean
Indicates whether to output responses in a streaming way:
true: Enable streaming (default).
false: Disable streaming.
"session_id": (Body Parameter)
The ID of session. If it is not provided, a new session will be generated.
"user_id": (Body parameter), string
The optional user-defined ID. Valid only when no session_id is provided.
Response
Success without session_id:

data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "Hi! I'm your assistant, what can I do for you?",
        "reference": {},
        "audio_binary": null,
        "id": null,
        "session_id": "b01eed84b85611efa0e90242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": true
}
Success with session_id:

data:{
    "code": 0,
    "data": {
        "answer": "I am an intelligent assistant designed to help answer questions by summarizing content from a",
        "reference": {},
        "audio_binary": null,
        "id": "a84c5dd4-97b4-4624-8c3b-974012c8000d",
        "session_id": "82b0ab2a9c1911ef9d870242ac120006"
    }
}
data:{
    "code": 0,
    "data": {
        "answer": "I am an intelligent assistant designed to help answer questions by summarizing content from a knowledge base. My responses are based on the information available in the knowledge base and",
        "reference": {},
        "audio_binary": null,
        "id": "a84c5dd4-97b4-4624-8c3b-974012c8000d",
        "session_id": "82b0ab2a9c1911ef9d870242ac120006"
    }
}
data:{
    "code": 0,
    "data": {
        "answer": "I am an intelligent assistant designed to help answer questions by summarizing content from a knowledge base. My responses are based on the information available in the knowledge base and any relevant chat history.",
        "reference": {},
        "audio_binary": null,
        "id": "a84c5dd4-97b4-4624-8c3b-974012c8000d",
        "session_id": "82b0ab2a9c1911ef9d870242ac120006"
    }
}
data:{
    "code": 0,
    "data": {
        "answer": "I am an intelligent assistant designed to help answer questions by summarizing content from a knowledge base ##0$$. My responses are based on the information available in the knowledge base and any relevant chat history.",
        "reference": {
            "total": 1,
            "chunks": [
                {
                    "id": "faf26c791128f2d5e821f822671063bd",
                    "content": "xxxxxxxx",
                    "document_id": "dd58f58e888511ef89c90242ac120006",
                    "document_name": "1.txt",
                    "dataset_id": "8e83e57a884611ef9d760242ac120006",
                    "image_id": "",
                    "similarity": 0.7,
                    "vector_similarity": 0.0,
                    "term_similarity": 1.0,
                    "positions": [
                        ""
                    ]
                }
            ],
            "doc_aggs": [
                {
                    "doc_name": "1.txt",
                    "doc_id": "dd58f58e888511ef89c90242ac120006",
                    "count": 1
                }
            ]
        },
        "prompt": "xxxxxxxxxxx",
        "id": "a84c5dd4-97b4-4624-8c3b-974012c8000d",
        "session_id": "82b0ab2a9c1911ef9d870242ac120006"
    }
}
data:{
    "code": 0,
    "data": true
}
Failure:

{
    "code": 102,
    "message": "Please input your question."
}
Create session with agent
POST /api/v1/agents/{agent_id}/sessions

Creates a session with an agent.

Request
Method: POST
URL: /api/v1/agents/{agent_id}/sessions
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
the required parameters:str
the optional parameters:str
"user_id": string
The optional user-defined ID.
Request example
If begin component in the agent doesn't have required parameters:

curl --request POST \
     --url http://{address}/api/v1/agents/{agent_id}/sessions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '{
     }'
If begin component in the agent has required parameters:

curl --request POST \
     --url http://{address}/api/v1/agents/{agent_id}/sessions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data '{
            "lang":"Japanese",
            "file":"Who are you"
     }'
Request parameters
agent_id: (Path parameter)
The ID of the associated agent.
Response
Success:

{
    "code": 0,
    "data": {
        "agent_id": "b4a39922b76611efaa1a0242ac120006",
        "dsl": {
            "answer": [],
            "components": {
                "Answer:GreenReadersDrum": {
                    "downstream": [],
                    "obj": {
                        "component_name": "Answer",
                        "inputs": [],
                        "output": null,
                        "params": {}
                    },
                    "upstream": []
                },
                "begin": {
                    "downstream": [],
                    "obj": {
                        "component_name": "Begin",
                        "inputs": [],
                        "output": {},
                        "params": {}
                    },
                    "upstream": []
                }
            },
            "embed_id": "",
            "graph": {
                "edges": [],
                "nodes": [
                    {
                        "data": {
                            "label": "Begin",
                            "name": "begin"
                        },
                        "dragging": false,
                        "height": 44,
                        "id": "begin",
                        "position": {
                            "x": 53.25688640427177,
                            "y": 198.37155679786412
                        },
                        "positionAbsolute": {
                            "x": 53.25688640427177,
                            "y": 198.37155679786412
                        },
                        "selected": false,
                        "sourcePosition": "left",
                        "targetPosition": "right",
                        "type": "beginNode",
                        "width": 200
                    },
                    {
                        "data": {
                            "form": {},
                            "label": "Answer",
                            "name": "dialog_0"
                        },
                        "dragging": false,
                        "height": 44,
                        "id": "Answer:GreenReadersDrum",
                        "position": {
                            "x": 360.43473114516974,
                            "y": 207.29298425089348
                        },
                        "positionAbsolute": {
                            "x": 360.43473114516974,
                            "y": 207.29298425089348
                        },
                        "selected": false,
                        "sourcePosition": "right",
                        "targetPosition": "left",
                        "type": "logicNode",
                        "width": 200
                    }
                ]
            },
            "history": [],
            "messages": [],
            "path": [
                [
                    "begin"
                ],
                []
            ],
            "reference": []
        },
        "id": "2581031eb7a311efb5200242ac120005",
        "message": [
            {
                "content": "Hi! I'm your smart assistant. What can I do for you?",
                "role": "assistant"
            }
        ],
        "source": "agent",
        "user_id": "69736c5e723611efb51b0242ac120007"
    }
}
Failure:

{
    "code": 102,
    "message": "Agent not found."
}
Converse with agent
POST /api/v1/agents/{agent_id}/completions

Asks a specified agent a question to start an AI-powered conversation.

:::tip NOTE

In streaming mode, not all responses include a reference, as this depends on the system's judgement.
In streaming mode, the last message is an empty message:
data:
{
  "code": 0,
  "data": true
}
:::

Request
Method: POST
URL: /api/v1/agents/{agent_id}/completions
Headers:
'content-Type: application/json'
'Authorization: Bearer <YOUR_API_KEY>'
Body:
"question": string
"stream": boolean
"session_id": string
"user_id": string(optional)
other parameters: string
Request example
If the begin component doesn't have parameters, the following code will create a session.

curl --request POST \
     --url http://{address}/api/v1/agents/{agent_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data-binary '
     {
     }'
If the begin component have parameters, the following code will create a session.

curl --request POST \
     --url http://{address}/api/v1/agents/{agent_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data-binary '
     {
          "lang":"English",
          "file":"How is the weather tomorrow?"
     }'
The following code will execute the completion process

curl --request POST \
     --url http://{address}/api/v1/agents/{agent_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer <YOUR_API_KEY>' \
     --data-binary '
     {
          "question": "Hello",
          "stream": true,
          "session_id": "cb2f385cb86211efa36e0242ac120005"
     }'
Request Parameters
agent_id: (Path parameter), string
The ID of the associated agent.
"question": (Body Parameter), string, Required
The question to start an AI-powered conversation.
"stream": (Body Parameter), boolean
Indicates whether to output responses in a streaming way:
true: Enable streaming (default).
false: Disable streaming.
"session_id": (Body Parameter)
The ID of the session. If it is not provided, a new session will be generated.
"user_id": (Body parameter), string
The optional user-defined ID. Valid only when no session_id is provided.
Other parameters: (Body Parameter)
The parameters in the begin component.
Response
success without session_id provided and with no parameters in the begin component:

data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "Hi! I'm your smart assistant. What can I do for you?",
        "reference": {},
        "id": "31e6091d-88d4-441b-ac65-eae1c055be7b",
        "session_id": "2987ad3eb85f11efb2a70242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": true
}
Success without session_id provided and with parameters in the begin component:

data:{
    "code": 0,
    "message": "",
    "data": {
        "session_id": "eacb36a0bdff11ef97120242ac120006",
        "answer": "",
        "reference": [],
        "param": [
            {
                "key": "lang",
                "name": "Target Language",
                "optional": false,
                "type": "line",
                "value": "English"
            },
            {
                "key": "file",
                "name": "Files",
                "optional": false,
                "type": "file",
                "value": "How is the weather tomorrow?"
            },
            {
                "key": "hhyt",
                "name": "hhty",
                "optional": true,
                "type": "line"
            }
        ]
    }
}
data:
Success with parameters in the begin component:

data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How is",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How is the",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How is the weather",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How is the weather tomorrow",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How is the weather tomorrow?",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": {
        "answer": "How is the weather tomorrow?",
        "reference": {},
        "id": "0379ac4c-b26b-4a44-8b77-99cebf313fdf",
        "session_id": "4399c7d0b86311efac5b0242ac120005"
    }
}
data:{
    "code": 0,
    "message": "",
    "data": true
}
Failure:

{
    "code": 102,
    "message": "`question` is required."
}
List agent sessions
GET /api/v1/agents/{agent_id}/sessions?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&id={session_id}&user_id={user_id}

Lists sessions associated with a specified agent.

Request
Method: GET
URL: /api/v1/agents/{agent_id}/sessions?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&id={session_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/agents/{agent_id}/sessions?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&id={session_id}&user_id={user_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>'
Request Parameters
agent_id: (Path parameter)
The ID of the associated agent.
page: (Filter parameter), integer
Specifies the page on which the sessions will be displayed. Defaults to 1.
page_size: (Filter parameter), integer
The number of sessions on each page. Defaults to 30.
orderby: (Filter parameter), string
The field by which sessions should be sorted. Available options:
create_time (default)
update_time
desc: (Filter parameter), boolean
Indicates whether the retrieved sessions should be sorted in descending order. Defaults to true.
id: (Filter parameter), string
The ID of the agent session to retrieve.
user_id: (Filter parameter), string
The optional user-defined ID passed in when creating session.
Response
Success:

{
    "code": 0,
    "data": {
        "agent_id": "e9e2b9c2b2f911ef801d0242ac120006",
        "dsl": {
            "answer": [],
            "components": {
                "Answer:OrangeTermsBurn": {
                    "downstream": [],
                    "obj": {
                        "component_name": "Answer",
                        "params": {}
                    },
                    "upstream": []
                },
                "Generate:SocialYearsRemain": {
                    "downstream": [],
                    "obj": {
                        "component_name": "Generate",
                        "params": {
                            "cite": true,
                            "frequency_penalty": 0.7,
                            "llm_id": "gpt-4o___OpenAI-API@OpenAI-API-Compatible",
                            "max_tokens": 256,
                            "message_history_window_size": 12,
                            "parameters": [],
                            "presence_penalty": 0.4,
                            "prompt": "Please summarize the following paragraph. Pay attention to the numbers and do not make things up. The paragraph is as follows:\n{input}\nThis is what you need to summarize.",
                            "temperature": 0.1,
                            "top_p": 0.3
                        }
                    },
                    "upstream": []
                },
                "begin": {
                    "downstream": [],
                    "obj": {
                        "component_name": "Begin",
                        "params": {}
                    },
                    "upstream": []
                }
            },
            "graph": {
                "edges": [],
                "nodes": [
                    {
                        "data": {
                            "label": "Begin",
                            "name": "begin"
                        },
                        "height": 44,
                        "id": "begin",
                        "position": {
                            "x": 50,
                            "y": 200
                        },
                        "sourcePosition": "left",
                        "targetPosition": "right",
                        "type": "beginNode",
                        "width": 200
                    },
                    {
                        "data": {
                            "form": {
                                "cite": true,
                                "frequencyPenaltyEnabled": true,
                                "frequency_penalty": 0.7,
                                "llm_id": "gpt-4o___OpenAI-API@OpenAI-API-Compatible",
                                "maxTokensEnabled": true,
                                "max_tokens": 256,
                                "message_history_window_size": 12,
                                "parameters": [],
                                "presencePenaltyEnabled": true,
                                "presence_penalty": 0.4,
                                "prompt": "Please summarize the following paragraph. Pay attention to the numbers and do not make things up. The paragraph is as follows:\n{input}\nThis is what you need to summarize.",
                                "temperature": 0.1,
                                "temperatureEnabled": true,
                                "topPEnabled": true,
                                "top_p": 0.3
                            },
                            "label": "Generate",
                            "name": "Generate Answer_0"
                        },
                        "dragging": false,
                        "height": 105,
                        "id": "Generate:SocialYearsRemain",
                        "position": {
                            "x": 561.3457829707513,
                            "y": 178.7211182312641
                        },
                        "positionAbsolute": {
                            "x": 561.3457829707513,
                            "y": 178.7211182312641
                        },
                        "selected": true,
                        "sourcePosition": "right",
                        "targetPosition": "left",
                        "type": "generateNode",
                        "width": 200
                    },
                    {
                        "data": {
                            "form": {},
                            "label": "Answer",
                            "name": "Dialogue_0"
                        },
                        "height": 44,
                        "id": "Answer:OrangeTermsBurn",
                        "position": {
                            "x": 317.2368194777658,
                            "y": 218.30635555445093
                        },
                        "sourcePosition": "right",
                        "targetPosition": "left",
                        "type": "logicNode",
                        "width": 200
                    }
                ]
            },
            "history": [],
            "messages": [],
            "path": [],
            "reference": []
        },
        "id": "792dde22b2fa11ef97550242ac120006",
        "message": [
            {
                "content": "Hi! I'm your smart assistant. What can I do for you?",
                "role": "assistant"
            }
        ],
        "source": "agent",
        "user_id": ""
    }
}
Failure:

{
    "code": 102,
    "message": "You don't own the agent ccd2f856b12311ef94ca0242ac1200052."
}
AGENT MANAGEMENT
List agents
GET /api/v1/agents?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={agent_name}&id={agent_id}

Lists agents.

Request
Method: GET
URL: /api/v1/agents?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={agent_name}&id={agent_id}
Headers:
'Authorization: Bearer <YOUR_API_KEY>'
Request example
curl --request GET \
     --url http://{address}/api/v1/agents?page={page}&page_size={page_size}&orderby={orderby}&desc={desc}&name={agent_name}&id={agent_id} \
     --header 'Authorization: Bearer <YOUR_API_KEY>'
Request parameters
page: (Filter parameter), integer
Specifies the page on which the agents will be displayed. Defaults to 1.
page_size: (Filter parameter), integer
The number of agents on each page. Defaults to 30.
orderby: (Filter parameter), string
The attribute by which the results are sorted. Available options:
create_time (default)
update_time
desc: (Filter parameter), boolean
Indicates whether the retrieved agents should be sorted in descending order. Defaults to true.
id: (Filter parameter), string
The ID of the agent to retrieve.
name: (Filter parameter), string
The name of the agent to retrieve.
Response
Success:

{
    "code": 0,
    "data": [
        {
            "avatar": null,
            "canvas_type": null,
            "create_date": "Thu, 05 Dec 2024 19:10:36 GMT",
            "create_time": 1733397036424,
            "description": null,
            "dsl": {
                "answer": [],
                "components": {
                    "begin": {
                        "downstream": [],
                        "obj": {
                            "component_name": "Begin",
                            "params": {}
                        },
                        "upstream": []
                    }
                },
                "graph": {
                    "edges": [],
                    "nodes": [
                        {
                            "data": {
                                "label": "Begin",
                                "name": "begin"
                            },
                            "height": 44,
                            "id": "begin",
                            "position": {
                                "x": 50,
                                "y": 200
                            },
                            "sourcePosition": "left",
                            "targetPosition": "right",
                            "type": "beginNode",
                            "width": 200
                        }
                    ]
                },
                "history": [],
                "messages": [],
                "path": [],
                "reference": []
            },
            "id": "8d9ca0e2b2f911ef9ca20242ac120006",
            "title": "123465",
            "update_date": "Thu, 05 Dec 2024 19:10:56 GMT",
            "update_time": 1733397056801,
            "user_id": "69736c5e723611efb51b0242ac120007"
        }
    ]
}
Failure:

{
    "code": 102,
    "message": "The agent doesn't exist."
}

================
File: compose-dev.yaml
================
services:
  app:
    entrypoint:
    - sleep
    - infinity
    image: docker/dev-environments-default:stable-1
    init: true
    volumes:
    - type: bind
      source: /var/run/docker.sock
      target: /var/run/docker.sock

================
File: pyproject.toml
================
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "ragflow-uploader"
version = "0.1.0"
description = "A robust file uploader and processor for RAGFlow with advanced progress tracking"
requires-python = ">=3.8"
authors = [
    { name = "Your Name", email = "your.email@example.com" }
]
dependencies = [
    "requests>=2.31.0",
    "tqdm>=4.66.1",
    "python-magic>=0.4.27",
    "python-magic-bin>=0.4.14; sys_platform == 'win32'",
    "watchdog>=3.0.0",
    "humanize>=4.9.0",
    "psutil>=5.9.8",
    "aiofiles>=23.2.1",
    "typing-extensions>=4.9.0"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",
    "black>=23.12.1",
    "isort>=5.13.2",
    "mypy>=1.8.0",
    "ruff>=0.1.9"
]

[tool.black]
line-length = 100
target-version = ["py38"]

[tool.isort]
profile = "black"
line_length = 100
multi_line_output = 3

[tool.mypy]
python_version = "3.8"
strict = true
ignore_missing_imports = true

[tool.ruff]
line-length = 100
target-version = "py38"
select = ["E", "F", "B", "I", "UP"]

[project.scripts]
ragflow-uploader = "ragflow_uploader:main"

================
File: ragflow_uploader.py
================
import os
import hashlib
import requests
import time
import fnmatch
import mimetypes
import magic  # for better file type detection
from typing import List, Set, Dict, Optional, Generator, Union, Tuple
from pathlib import Path
from tqdm import tqdm
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import asyncio
from dataclasses import dataclass, field
import json
from functools import partial
from datetime import datetime, timedelta
import humanize  # for human-readable sizes and times
import psutil  # for system resource monitoring
from collections import deque

# Configure logging with file output
log_file = 'ragflow_uploader.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

@dataclass
class ProcessingMetrics:
    """
A class to track and convert real-time processing metrics into a dictionary format.

Attributes:
    cpu_percent (float): The percentage of CPU usage.
    memory_percent (float): The percentage of memory usage.
    disk_io_read (int): The number of bytes read from disk.
    disk_io_write (int): The number of bytes written to disk.
    network_sent (int): The number of bytes sent over the network.
    network_recv (int): The number of bytes received over the network.

Methods:
    to_dict: Converts the metrics into a dictionary with human-readable values.
"""
    cpu_percent: float = 0.0
    memory_percent: float = 0.0
    disk_io_read: int = 0
    disk_io_write: int = 0
    network_sent: int = 0
    network_recv: int = 0
    
    def to_dict(self) -> dict:
        return {
            'cpu_percent': f"{self.cpu_percent:.1f}%",
            'memory_percent': f"{self.memory_percent:.1f}%",
            'disk_io_read': humanize.naturalsize(self.disk_io_read),
            'disk_io_write': humanize.naturalsize(self.disk_io_write),
            'network_sent': humanize.naturalsize(self.network_sent),
            'network_recv': humanize.naturalsize(self.network_recv)
        }

@dataclass
class FileStats:
    """Enhanced statistics for file processing with historical data"""
    total_size: int = 0
    processed_size: int = 0
    successful_count: int = 0
    failed_count: int = 0
    skipped_count: int = 0
    retry_count: int = 0
    start_time: Optional[datetime] = None
    last_error: Optional[str] = None
    error_files: List[str] = field(default_factory=list)
    current_file: Optional[str] = None
    
    # Historical data for graphs
    speed_history: deque = field(default_factory=lambda: deque(maxlen=100))
    success_rate_history: deque = field(default_factory=lambda: deque(maxlen=100))
    metrics_history: deque = field(default_factory=lambda: deque(maxlen=100))
    
    # File type statistics
    extension_stats: Dict[str, int] = field(default_factory=dict)
    mime_type_stats: Dict[str, int] = field(default_factory=dict)
    size_distribution: Dict[str, int] = field(default_factory=dict)
    
    def __post_init__(self):
        self.error_files = []
        self.speed_history = deque(maxlen=100)
        self.success_rate_history = deque(maxlen=100)
        self.metrics_history = deque(maxlen=100)
        self.extension_stats = {}
        self.mime_type_stats = {}
        self.size_distribution = {
            '0-1MB': 0,
            '1-10MB': 0,
            '10-100MB': 0,
            '100MB-1GB': 0,
            '>1GB': 0
        }
        self.current_metrics = ProcessingMetrics()
        self._last_update = time.time()
        self._last_processed_size = 0
    
    def update_metrics(self):
        """Update system metrics"""
        self.current_metrics.cpu_percent = psutil.cpu_percent()
        self.current_metrics.memory_percent = psutil.virtual_memory().percent
        
        disk_io = psutil.disk_io_counters()
        self.current_metrics.disk_io_read = disk_io.read_bytes
        self.current_metrics.disk_io_write = disk_io.write_bytes
        
        net_io = psutil.net_io_counters()
        self.current_metrics.network_sent = net_io.bytes_sent
        self.current_metrics.network_recv = net_io.bytes_recv
        
        self.metrics_history.append(self.current_metrics)
    
    def update_file_stats(self, file_path: Path, mime_type: str):
        """Update file statistics"""
        ext = file_path.suffix.lower()
        size = file_path.stat().st_size
        
        # Update extension stats
        self.extension_stats[ext] = self.extension_stats.get(ext, 0) + 1
        
        # Update MIME type stats
        self.mime_type_stats[mime_type] = self.mime_type_stats.get(mime_type, 0) + 1
        
        # Update size distribution
        size_mb = size / (1024 * 1024)
        if size_mb <= 1:
            self.size_distribution['0-1MB'] += 1
        elif size_mb <= 10:
            self.size_distribution['1-10MB'] += 1
        elif size_mb <= 100:
            self.size_distribution['10-100MB'] += 1
        elif size_mb <= 1024:
            self.size_distribution['100MB-1GB'] += 1
        else:
            self.size_distribution['>1GB'] += 1
    
    @property
    def elapsed_time(self) -> float:
        if not self.start_time:
            return 0
        return (datetime.now() - self.start_time).total_seconds()
    
    @property
    def processing_speed(self) -> float:
        now = time.time()
        time_diff = now - self._last_update
        size_diff = self.processed_size - self._last_processed_size
        
        if time_diff > 0:
            speed = size_diff / (1024 * 1024 * time_diff)  # MB/s
            self.speed_history.append(speed)
            self._last_update = now
            self._last_processed_size = self.processed_size
            return speed
        return 0
    
    @property
    def average_speed(self) -> float:
        """Calculate moving average speed"""
        if not self.speed_history:
            return 0
        return sum(self.speed_history) / len(self.speed_history)
    
    @property
    def success_rate(self) -> float:
        total = self.successful_count + self.failed_count
        if total == 0:
            return 100.0
        rate = (self.successful_count / total) * 100
        self.success_rate_history.append(rate)
        return rate
    
    @property
    def estimated_time_remaining(self) -> timedelta:
        if not self.average_speed or not self.total_size:
            return timedelta(0)
        remaining_size = self.total_size - self.processed_size
        remaining_seconds = remaining_size / (self.average_speed * 1024 * 1024)
        return timedelta(seconds=int(remaining_seconds))
    
    def format_progress(self) -> str:
        """Format progress for display"""
        return (
            f"\nProgress Report:"
            f"\n{'='*50}"
            f"\nFiles:"
            f"\n - Processing: {self.current_file or 'None'}"
            f"\n - Successful: {self.successful_count}"
            f"\n - Failed: {self.failed_count}"
            f"\n - Skipped: {self.skipped_count}"
            f"\n - Success Rate: {self.success_rate:.1f}%"
            f"\n"
            f"\nSize:"
            f"\n - Total: {humanize.naturalsize(self.total_size)}"
            f"\n - Processed: {humanize.naturalsize(self.processed_size)}"
            f"\n - Remaining: {humanize.naturalsize(self.total_size - self.processed_size)}"
            f"\n"
            f"\nSpeed:"
            f"\n - Current: {self.processing_speed:.1f} MB/s"
            f"\n - Average: {self.average_speed:.1f} MB/s"
            f"\n - Time Elapsed: {humanize.naturaldelta(self.elapsed_time)}"
            f"\n - Time Remaining: {humanize.naturaldelta(self.estimated_time_remaining)}"
            f"\n"
            f"\nSystem Metrics:"
            f"\n - CPU Usage: {self.current_metrics.cpu_percent:.1f}%"
            f"\n - Memory Usage: {self.current_metrics.memory_percent:.1f}%"
            f"\n - Disk Read: {humanize.naturalsize(self.current_metrics.disk_io_read)}"
            f"\n - Disk Write: {humanize.naturalsize(self.current_metrics.disk_io_write)}"
            f"\n - Network Sent: {humanize.naturalsize(self.current_metrics.network_sent)}"
            f"\n - Network Received: {humanize.naturalsize(self.current_metrics.network_recv)}"
            f"\n{'='*50}"
        )
    
    def to_dict(self) -> dict:
        """Convert stats to JSON-serializable dict with enhanced metrics"""
        return {
            'files': {
                'total': self.successful_count + self.failed_count + self.skipped_count,
                'successful': self.successful_count,
                'failed': self.failed_count,
                'skipped': self.skipped_count,
                'success_rate': f"{self.success_rate:.1f}%",
                'error_files': self.error_files,
                'current_file': self.current_file
            },
            'size': {
                'total': humanize.naturalsize(self.total_size),
                'processed': humanize.naturalsize(self.processed_size),
                'remaining': humanize.naturalsize(self.total_size - self.processed_size)
            },
            'speed': {
                'current': f"{self.processing_speed:.1f} MB/s",
                'average': f"{self.average_speed:.1f} MB/s",
                'history': list(self.speed_history)
            },
            'time': {
                'elapsed': humanize.naturaldelta(self.elapsed_time),
                'remaining': humanize.naturaldelta(self.estimated_time_remaining)
            },
            'metrics': self.current_metrics.to_dict(),
            'file_stats': {
                'extensions': self.extension_stats,
                'mime_types': self.mime_type_stats,
                'size_distribution': self.size_distribution
            }
        }

class ProgressDisplay:
    """Handles progress display and updates"""
    
    def __init__(self, stats: FileStats, refresh_rate: float = 1.0):
        self.stats = stats
        self.refresh_rate = refresh_rate
        self._last_display = 0
    
    def update(self, force: bool = False):
        """Update progress display if needed"""
        now = time.time()
        if force or (now - self._last_display) >= self.refresh_rate:
            self.stats.update_metrics()
            print(self.stats.format_progress())
            self._last_display = now

class FileProcessor:
    """Enhanced file filtering and stats collection"""
    
    def __init__(
        self,
        allowed_extensions: Optional[Set[str]] = None,
        exclude_dirs: Optional[List[str]] = None,
        exclude_patterns: Optional[List[str]] = None,
        min_size: Optional[int] = None,
        max_size: Optional[int] = None,
        content_patterns: Optional[List[str]] = None,
        mime_types: Optional[List[str]] = None,
        modified_after: Optional[datetime] = None,
        modified_before: Optional[datetime] = None,
        max_depth: Optional[int] = None,
        include_hidden: bool = False,
        min_text_ratio: Optional[float] = None,
        encoding: str = 'utf-8',
        backup_dir: Optional[str] = None
    ):
        self.allowed_extensions = allowed_extensions
        self.exclude_dirs = exclude_dirs or []
        self.exclude_patterns = exclude_patterns or []
        self.min_size = min_size
        self.max_size = max_size
        self.content_patterns = content_patterns
        self.mime_types = set(mime_types) if mime_types else None
        self.modified_after = modified_after
        self.modified_before = modified_before
        self.max_depth = max_depth
        self.include_hidden = include_hidden
        self.min_text_ratio = min_text_ratio
        self.encoding = encoding
        self.backup_dir = backup_dir
        self.stats = FileStats()
        self.magic = magic.Magic(mime=True)
        
    def is_hidden(self, path: Path) -> bool:
        """Check if file or any parent directory is hidden"""
        parts = path.parts
        return any(part.startswith('.') for part in parts)
    
    def get_relative_depth(self, path: Path, root: Path) -> int:
        """Calculate relative directory depth"""
        return len(path.relative_to(root).parts)
    
    def is_text_file(self, file_path: Path) -> bool:
        """Check if file is text and meets minimum text ratio"""
        try:
            mime_type = self.magic.from_file(str(file_path))
            if not mime_type.startswith('text/'):
                return False

            if self.min_text_ratio:
                with open(file_path, 'rb') as f:
                    content = f.read()
                    text_chars = sum(32 <= c <= 126 or c in {9, 10, 13} for c in content)
                    ratio = text_chars / len(content)
                    return ratio >= self.min_text_ratio
            return True
        except Exception:
            return False
    
    def create_backup(self, file_path: Path) -> Optional[Path]:
        """Create backup of file before processing"""
        if not self.backup_dir:
            return None
            
        try:
            backup_path = Path(self.backup_dir) / file_path.name
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            import shutil
            shutil.copy2(file_path, backup_path)
            return backup_path
        except Exception as e:
            logging.warning(f"Failed to create backup of {file_path}: {e}")
            return None
        
    def should_process_file(self, file_path: Path, root_dir: Path = None) -> bool:
        """Enhanced file filtering"""
        try:
            # Hidden file check
            if not self.include_hidden and self.is_hidden(file_path):
                return False

            # Depth check
            if root_dir and self.max_depth is not None and self.get_relative_depth(file_path, root_dir) > self.max_depth:
                return False

            # Basic checks
            if self.allowed_extensions and file_path.suffix.lower() not in self.allowed_extensions:
                return False

            if any(fnmatch.fnmatch(file_path.name, pattern) for pattern in self.exclude_patterns):
                return False

            # Size check
            file_size = file_path.stat().st_size
            if self.min_size and file_size < self.min_size:
                return False
            if self.max_size and file_size > self.max_size:
                return False

            # Modification time check
            mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
            if self.modified_after and mtime < self.modified_after:
                return False
            if self.modified_before and mtime > self.modified_before:
                return False

            # MIME type check
            if self.mime_types:
                mime_type = self.magic.from_file(str(file_path))
                if not any(mime_type.startswith(m) for m in self.mime_types):
                    return False

            # Text ratio check
            if self.min_text_ratio and not self.is_text_file(file_path):
                return False

            # Content pattern check
            if self.content_patterns:
                try:
                    with open(file_path, 'r', encoding=self.encoding, errors='ignore') as f:
                        content = f.read()
                        if all(
                            pattern not in content
                            for pattern in self.content_patterns
                        ):
                            return False
                except UnicodeDecodeError:
                    return False

            return True
        except Exception as e:
            logging.warning(f"Error checking file {file_path}: {str(e)}")
            return False

class RetryPolicy:
    """Configurable retry policy for failed operations"""
    
    def __init__(
        self,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        exponential_base: float = 2.0,
        jitter: float = 0.1
    ):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter
    
    def get_delay(self, attempt: int) -> float:
        """Calculate delay with exponential backoff and jitter"""
        import random
        delay = min(
            self.base_delay * (self.exponential_base ** attempt),
            self.max_delay
        )
        jitter_range = delay * self.jitter
        return delay + random.uniform(-jitter_range, jitter_range)

class DirectoryWatcher(FileSystemEventHandler):
    """Watches directory for changes and triggers processing"""
    
    def __init__(self, uploader, dataset_name: str, processor: FileProcessor):
        self.uploader = uploader
        self.dataset_name = dataset_name
        self.processor = processor
        self.processing_queue = asyncio.Queue()
        self.dataset_id = None
        
    async def start(self, path: str):
        """Start watching directory"""
        self.dataset_id = self.uploader.create_dataset(self.dataset_name)
        
        # Start observer
        observer = Observer()
        observer.schedule(self, path, recursive=True)
        observer.start()
        
        try:
            while True:
                # Process any files in queue
                try:
                    file_path = await asyncio.wait_for(self.processing_queue.get(), timeout=1.0)
                    await self.process_file(file_path)
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    logging.error(f"Error processing file: {str(e)}")
        finally:
            observer.stop()
            observer.join()
            
    async def process_file(self, file_path: Path):
        """Process a single file"""
        if not self.processor.should_process_file(file_path):
            return
            
        try:
            await self.uploader.upload_and_process_file(
                self.dataset_id,
                file_path,
                update_stats=self.processor.stats
            )
        except Exception as e:
            logging.error(f"Error processing {file_path}: {str(e)}")
            
    def on_created(self, event):
        if not event.is_directory:
            self.processing_queue.put_nowait(Path(event.src_path))
            
    def on_modified(self, event):
        if not event.is_directory:
            self.processing_queue.put_nowait(Path(event.src_path))

class RAGFlowUploader:
    """Pipeline for uploading local directories to RAGFlow with progress tracking and duplicate handling"""
    
    # Default supported file extensions
    DEFAULT_SUPPORTED_EXTENSIONS = {
        '.pdf', '.docx', '.txt', '.md', '.csv',
        '.json', '.xml', '.html', '.htm'
    }
    
    def __init__(
        self,
        api_key: str,
        base_url: str = "http://localhost",
        max_retries: int = 3,
        retry_delay: float = 1.0,
        max_workers: int = 4,
        hash_algorithm: str = 'sha256',
        retry_policy: RetryPolicy = None
    ):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.max_workers = max_workers
        self.logger = logging.getLogger(__name__)
        self.hash_algorithm = getattr(hashlib, hash_algorithm)
        self.retry_policy = retry_policy
        
    def _make_request(
        self,
        method: str,
        endpoint: str,
        **kwargs
    ) -> dict:
        """Make HTTP request with retries and error handling"""
        url = f"{self.base_url}/api/v1/{endpoint.lstrip('/')}"
        
        for attempt in range(self.max_retries):
            try:
                response = requests.request(method, url, **kwargs)
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                if attempt == self.max_retries - 1:
                    raise
                self.logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                time.sleep(self.retry_delay * (attempt + 1))

    def create_dataset(self, name: str) -> str:
        """Create or get existing dataset"""
        self.logger.info(f"Creating/retrieving dataset: {name}")
        
        try:
            result = self._make_request(
                "POST",
                "datasets",
                headers=self.headers,
                json={"name": name}
            )
            dataset_id = result['data']['id']
            self.logger.info(f"Created new dataset: {name} (ID: {dataset_id})")
            return dataset_id
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 409:  # Handle duplicate dataset
                datasets = self._make_request("GET", "datasets", headers=self.headers)
                dataset_id = next(d['id'] for d in datasets['data'] if d['name'] == name)
                self.logger.info(f"Using existing dataset: {name} (ID: {dataset_id})")
                return dataset_id
            raise

    def get_existing_documents(self, dataset_id: str) -> Dict[str, Dict]:
        """Get map of existing documents with metadata for smarter duplicate checking"""
        self.logger.info(f"Fetching existing documents for dataset: {dataset_id}")

        result = self._make_request(
            "GET",
            f"datasets/{dataset_id}/documents",
            headers=self.headers
        )

        docs = {
            doc['name']: {
                'id': doc['id'],
                'size': doc.get('size'),
                'update_time': doc.get('update_time'),
                'hash': doc.get('hash'),  # Store hash if available
            }
            for doc in result['data'].get('docs', [])
        }
        self.logger.info(f"Found {len(docs)} existing documents")
        return docs

    def calculate_file_hash(self, file_path: Path, chunk_size: int = 8192) -> str:
        """Calculate file hash for duplicate detection"""
        hash_obj = self.hash_algorithm()
        
        total_size = file_path.stat().st_size
        with tqdm(total=total_size, unit='B', unit_scale=True, desc=f"Hashing {file_path.name}") as pbar:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(chunk_size), b""):
                    hash_obj.update(chunk)
                    pbar.update(len(chunk))
                    
        return hash_obj.hexdigest()

    def should_upload_file(
        self,
        file_path: Path,
        existing_docs: Dict[str, Dict],
        force_hash_check: bool = False
    ) -> bool:
        """Determine if file should be uploaded based on metadata and optionally hash"""
        if file_path.name not in existing_docs:
            return True
            
        existing = existing_docs[file_path.name]
        file_stat = file_path.stat()
        
        # Quick checks first
        if file_stat.st_size != existing.get('size'):
            return True
            
        file_mtime = int(file_stat.st_mtime * 1000)
        if abs(file_mtime - existing.get('update_time', 0)) > 1000:
            return True
            
        # Optional hash check
        if force_hash_check or existing.get('hash'):
            file_hash = self.calculate_file_hash(file_path)
            return file_hash != existing.get('hash')
            
        return False

    async def upload_and_process_file(
        self,
        dataset_id: str,
        file_path: Path,
        pbar: Optional[tqdm] = None,
        update_stats: Optional[FileStats] = None
    ) -> Optional[str]:
        """Upload file with progress tracking and stats updates"""
        try:
            if update_stats:
                update_stats.total_size += file_path.stat().st_size
                
            result = self._make_request(
                "POST",
                f"datasets/{dataset_id}/documents",
                headers={'Authorization': self.headers['Authorization']},
                files={'file': (file_path.name, open(file_path, 'rb'))}
            )
            
            doc_id = result['data'][0]['id']
            
            if pbar:
                pbar.update(1)
                pbar.set_postfix(
                    file=file_path.name,
                    status="OK",
                    speed=f"{update_stats.processing_speed:.1f}MB/s" if update_stats else ""
                )
                
            if update_stats:
                update_stats.successful_count += 1
                update_stats.processed_size += file_path.stat().st_size
                
            return doc_id
            
        except Exception as e:
            self.logger.error(f"Error uploading {file_path}: {str(e)}")
            if pbar:
                pbar.set_postfix(file=file_path.name, status=f"Failed: {str(e)}")
            if update_stats:
                update_stats.failed_count += 1
            return None

    async def _process_files_batch(
        self,
        dataset_id: str,
        files: List[Path],
        pbar: tqdm,
        processor: FileProcessor
    ) -> List[str]:
        """Process a batch of files concurrently"""
        successful_uploads = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                (asyncio.get_event_loop().run_in_executor(
                    executor,
                    self.upload_and_process_file,
                    dataset_id,
                    file_path,
                    pbar,
                    processor.stats
                ), file_path)
                for file_path in files
            ]
            
            for future, file_path in futures:
                try:
                    if doc_id := await future:
                        successful_uploads.append(doc_id)
                except Exception as e:
                    self.logger.error(f"Failed to process {file_path}: {str(e)}")
                    processor.stats.failed_count += 1
                    
        return successful_uploads

    async def _start_document_processing(self, dataset_id: str, doc_ids: List[str]):
        """Start processing uploaded documents"""
        if not doc_ids:
            return
            
        self.logger.info(f"Starting document processing for {len(doc_ids)} files")
        try:
            self._make_request(
                "POST",
                f"datasets/{dataset_id}/chunks",
                headers=self.headers,
                json={"document_ids": doc_ids}
            )
            self.logger.info("Document processing started successfully")
        except Exception as e:
            self.logger.error(f"Failed to start document processing: {str(e)}")

    def _collect_files(
        self,
        root_dir: str,
        processor: FileProcessor,
        existing_docs: Dict[str, Dict],
        force_hash_check: bool
    ) -> List[Path]:
        """Collect files to process with filtering"""
        files_to_process = []
        
        for root, dirs, files in os.walk(root_dir):
            dirs[:] = [d for d in dirs if d not in (processor.exclude_dirs or [])]
            
            for file in files:
                file_path = Path(root) / file
                
                if not processor.should_process_file(file_path):
                    processor.stats.skipped_count += 1
                    continue
                    
                if self.should_upload_file(file_path, existing_docs, force_hash_check):
                    files_to_process.append(file_path)
                else:
                    processor.stats.skipped_count += 1
                    
        return files_to_process

    def _log_final_stats(self, processor: FileProcessor, files_count: int):
        """Log final processing statistics"""
        self.logger.info(
            f"\nProcessing complete:"
            f"\n - Total files: {files_count}"
            f"\n - Successful: {processor.stats.successful_count}"
            f"\n - Failed: {processor.stats.failed_count}"
            f"\n - Skipped: {processor.stats.skipped_count}"
            f"\n - Total size: {processor.stats.total_size / (1024*1024):.1f}MB"
            f"\n - Average speed: {processor.stats.processing_speed:.1f}MB/s"
            f"\n - Time taken: {processor.stats.elapsed_time:.1f}s"
        )

    async def process_directory(
        self,
        dataset_name: str,
        root_dir: str,
        processor: Optional[FileProcessor] = None,
        watch: bool = False,
        force_hash_check: bool = False
    ):
        """Process a directory recursively, with optional watching"""
        processor = processor or FileProcessor(allowed_extensions=self.DEFAULT_SUPPORTED_EXTENSIONS)
        processor.stats.start_time = datetime.now()
        
        self.logger.info(f"Starting directory processing: {root_dir}")
        
        dataset_id = self.create_dataset(dataset_name)
        if not dataset_id:
            self.logger.error("Failed to create/get dataset")
            return

        existing_docs = self.get_existing_documents(dataset_id)
        files_to_process = self._collect_files(root_dir, processor, existing_docs, force_hash_check)

        if not files_to_process:
            self.logger.info("No new files to process")
            if watch:
                watcher = DirectoryWatcher(self, dataset_name, processor)
                await watcher.start(root_dir)
            return

        progress = ProgressDisplay(processor.stats)
        with tqdm(
            total=len(files_to_process),
            desc="Uploading files",
            unit='file',
            postfix={'speed': '0MB/s'}
        ) as pbar:
            successful_uploads = await self._process_files_batch(dataset_id, files_to_process, pbar, processor)
            progress.update(force=True)

        self._log_final_stats(processor, len(files_to_process))
        await self._start_document_processing(dataset_id, successful_uploads)
        
        if watch:
            watcher = DirectoryWatcher(self, dataset_name, processor)
            await watcher.start(root_dir)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description='RAGFlow Directory Pipeline - Upload and process local files'
    )
    parser.add_argument('directory', help='Directory to process')
    parser.add_argument(
        '--dataset',
        default='auto_upload',
        help='Dataset name (default: auto_upload)'
    )
    parser.add_argument(
        '--extensions',
        nargs='*',
        help='Allowed file extensions (default: pdf,docx,txt,md,csv,json,xml,html)'
    )
    parser.add_argument(
        '--exclude-dirs',
        nargs='*',
        help='Directory names to exclude'
    )
    parser.add_argument(
        '--exclude-patterns',
        nargs='*',
        help='Glob patterns to exclude (e.g. "*.tmp" "backup_*")'
    )
    parser.add_argument(
        '--min-size',
        type=int,
        help='Minimum file size in bytes'
    )
    parser.add_argument(
        '--max-size',
        type=int,
        help='Maximum file size in bytes'
    )
    parser.add_argument(
        '--content-patterns',
        nargs='*',
        help='Only process files containing these patterns'
    )
    parser.add_argument(
        '--max-workers',
        type=int,
        default=4,
        help='Maximum number of concurrent uploads (default: 4)'
    )
    parser.add_argument(
        '--base-url',
        default='http://localhost',
        help='RAGFlow API base URL (default: http://localhost)'
    )
    parser.add_argument(
        '--watch',
        action='store_true',
        help='Watch directory for changes'
    )
    parser.add_argument(
        '--force-hash',
        action='store_true',
        help='Force hash checking for all files'
    )
    parser.add_argument(
        '--hash-algorithm',
        default='sha256',
        choices=['md5', 'sha1', 'sha256', 'sha512'],
        help='Hash algorithm for duplicate detection (default: sha256)'
    )
    parser.add_argument(
        '--mime-types',
        nargs='*',
        help='Only process files of these MIME types (e.g. "text/plain" "application/pdf")'
    )
    parser.add_argument(
        '--modified-after',
        type=lambda s: datetime.fromisoformat(s),
        help='Only process files modified after this date (ISO format)'
    )
    parser.add_argument(
        '--modified-before',
        type=lambda s: datetime.fromisoformat(s),
        help='Only process files modified before this date (ISO format)'
    )
    parser.add_argument(
        '--max-depth',
        type=int,
        help='Maximum directory depth to process'
    )
    parser.add_argument(
        '--include-hidden',
        action='store_true',
        help='Include hidden files and directories'
    )
    parser.add_argument(
        '--min-text-ratio',
        type=float,
        help='Minimum ratio of text characters for text files'
    )
    parser.add_argument(
        '--encoding',
        default='utf-8',
        help='Encoding for text file operations (default: utf-8)'
    )
    parser.add_argument(
        '--backup-dir',
        help='Directory to store file backups before processing'
    )
    parser.add_argument(
        '--retry-policy',
        type=json.loads,
        help='JSON retry policy configuration'
    )
    parser.add_argument(
        '--stats-file',
        help='File to save processing statistics'
    )
    parser.add_argument(
        '--progress-refresh',
        type=float,
        default=1.0,
        help='Progress display refresh rate in seconds (default: 1.0)'
    )
    parser.add_argument(
        '--no-progress',
        action='store_true',
        help='Disable detailed progress display'
    )

    args = parser.parse_args()

    # Get API key from environment
    api_key = os.getenv('RAGFLOW_API_KEY')
    if not api_key:
        raise ValueError("Missing RAGFLOW_API_KEY environment variable")

    # Convert extensions to set with dots
    extensions = (
        {f".{ext.lstrip('.')}" for ext in args.extensions}
        if args.extensions
        else None
    )

    # Create file processor with enhanced options
    processor = FileProcessor(
        allowed_extensions=extensions,
        exclude_dirs=args.exclude_dirs,
        exclude_patterns=args.exclude_patterns,
        min_size=args.min_size,
        max_size=args.max_size,
        content_patterns=args.content_patterns,
        mime_types=args.mime_types,
        modified_after=args.modified_after,
        modified_before=args.modified_before,
        max_depth=args.max_depth,
        include_hidden=args.include_hidden,
        min_text_ratio=args.min_text_ratio,
        encoding=args.encoding,
        backup_dir=args.backup_dir
    )

    # Configure retry policy
    retry_policy = RetryPolicy(**(args.retry_policy or {}))

    # Create uploader with retry policy
    uploader = RAGFlowUploader(
        api_key=api_key,
        base_url=args.base_url,
        max_workers=args.max_workers,
        hash_algorithm=args.hash_algorithm,
        retry_policy=retry_policy
    )

    # Run async process_directory
    try:
        asyncio.run(uploader.process_directory(
            dataset_name=args.dataset,
            root_dir=args.directory,
            processor=processor,
            watch=args.watch,
            force_hash_check=args.force_hash
        ))
    finally:
        # Save final stats if requested
        if args.stats_file:
            with open(args.stats_file, 'w') as f:
                json.dump(processor.stats.to_dict(), f, indent=2)

================
File: requirements.txt
================
# Core dependencies
requests>=2.31.0
tqdm>=4.66.1
python-magic>=0.4.27
python-magic-bin>=0.4.14; sys_platform == 'win32'  # Required for Windows
watchdog>=3.0.0
humanize>=4.9.0
psutil>=5.9.8
aiofiles>=23.2.1
typing-extensions>=4.9.0

# Development dependencies
pytest>=7.4.0
pytest-asyncio>=0.23.0
pytest-cov>=4.1.0
black>=23.12.1
isort>=5.13.2
mypy>=1.8.0
ruff>=0.1.9
